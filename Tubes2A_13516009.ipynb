{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATASET TRAIN AND TEST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv('data/tubes2_HeartDisease_train.csv')\n",
    "test_set = pd.read_csv('data/tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# DATASET ClEANING\n",
    "# Column Dataset Train\n",
    "## Column 4 = 46 kosong\n",
    "## Column 5 = 24\n",
    "## Column 6 = 78\n",
    "## Column 7 = 1 <- nilainya jadiin 0\n",
    "## Column 8 = 43\n",
    "## Column 9 = 43 angina\n",
    "## Column 10 = 48 ST depression\n",
    "## Column 11 = 261 \n",
    "## Column 12 = 513 BUANG\n",
    "## Column 13 = 407 BUANG\n",
    "import math\n",
    "dataset = dataset.drop(columns=['Column12', 'Column13'], axis=1)\n",
    "test_set = test_set.drop(columns=['Column12', 'Column13'])\n",
    "\n",
    "data_len = len(dataset['Column7'])\n",
    "for i in range(0, data_len):\n",
    "    if dataset['Column7'][i] == '?' or math.isnan(float(dataset['Column7'][i])):\n",
    "        dataset['Column7'][i] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus row yang ?-nya lebih dari 5\n",
    "#deleterow = []\n",
    "#for i in range(0, 779):\n",
    "#    count = 0\n",
    "#    for j in range(0, 12):\n",
    "#        if dataset.iloc[i][j] == '?':\n",
    "#            count += 1\n",
    "#    if count >= 5:\n",
    "#        deleterow.append(i)\n",
    "#print(deleterow)\n",
    "\n",
    "#dataset = dataset.drop(deleterow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Replace '?' data with mean value\n",
    "dataset['Column4'][15]\n",
    "avgList = []\n",
    "for i in range(3, 12):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for j in range(0, 736):\n",
    "        if dataset.iloc[j][i] != '?':\n",
    "            sum += float(dataset.iloc[j][i])\n",
    "            count += 1\n",
    "    avg = sum/count\n",
    "    avgList.append(round(avg))\n",
    "col = 'Column'\n",
    "for i in range(4, 12):\n",
    "    column = col + str(i)\n",
    "    for j in range(0, 736):\n",
    "        if dataset[column][j] == '?':\n",
    "            dataset[column][j] = avgList[i-4]\n",
    "    for k in range(0, len(test_set[column])):\n",
    "        if test_set[column][k] == '?':\n",
    "            test_set[column][k] = avgList[i-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset yang sudah dibersihin\n",
    "dataset\n",
    "#test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def generate_fold(k, dataset):\n",
    "    dataset_size = len(dataset)\n",
    "    fold = [[] for i in range (k)]\n",
    "    nums = [i for i in range(k)]\n",
    "    sizes = [0 for i in range(k)]\n",
    "    normal_size = math.floor(dataset_size / k)\n",
    "    max_size = math.ceil(dataset_size / k)\n",
    "    size = max_size\n",
    "    max_size_counter = dataset_size % k\n",
    "    \n",
    "    for val in dataset:\n",
    "        idx = choice(nums)\n",
    "        fold[idx].append(val)\n",
    "        sizes[idx] += 1\n",
    "        \n",
    "        if sizes[idx] == size:\n",
    "            nums.remove(idx)\n",
    "        \n",
    "        if sizes[idx] == max_size:\n",
    "            max_size_counter -= 1\n",
    "            if max_size_counter == 0:\n",
    "                size = normal_size\n",
    "                \n",
    "                temp = []\n",
    "                for num in nums:\n",
    "                    if sizes[num] == size:\n",
    "                        temp.append(num)\n",
    "                for t in temp:\n",
    "                    nums.remove(t)\n",
    "    return fold\n",
    "\n",
    "def seperate(dataset):\n",
    "    params = []\n",
    "    lables = []\n",
    "    datasize = len(dataset[0])\n",
    "    for data in dataset:\n",
    "        params.append(data[:datasize-1])\n",
    "        lables.append(data[datasize-1])\n",
    "    return params, lables\n",
    "\n",
    "def parse_dataset(frame):\n",
    "    dataset = []\n",
    "    for index, row in frame.iterrows():\n",
    "        dataset.append(row.values.tolist())\n",
    "    return dataset\n",
    "\n",
    "def get_trainingset(index, folds):\n",
    "    training_set = []\n",
    "    for i in range(len(folds)):\n",
    "        if i != index:\n",
    "            for data in folds[i]:\n",
    "                training_set.append(data)\n",
    "    return training_set\n",
    "\n",
    "def pseudo_clean(dataset):\n",
    "    cleaned = []\n",
    "    idx = 0\n",
    "    for data in dataset:\n",
    "        row = []\n",
    "        for val in data:\n",
    "            row.append(float(val))\n",
    "        cleaned.append(row)\n",
    "        idx += 1\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "def train(trainer, folds):\n",
    "    classifiers = []\n",
    "    for i in range (len(folds)):\n",
    "        training_set = get_trainingset(i, folds)\n",
    "        training_params, training_lables = seperate(training_set)\n",
    "        classifiers.append(trainer.fit(training_params, training_lables))\n",
    "    return classifiers\n",
    "\n",
    "def train_naive_bayes(folds):\n",
    "    return train(GaussianNB(), folds)\n",
    "\n",
    "def train_decision_tree(folds):\n",
    "    return train(tree.DecisionTreeClassifier(), folds)\n",
    "\n",
    "def train_knn(neighbor, folds):\n",
    "    return train(KNeighborsClassifier(neighbor), folds)\n",
    "\n",
    "def train_mlp(folds):\n",
    "    return train(MLPClassifier(learning_rate_init=0.01, max_iter=300), folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = parse_dataset(dataset)\n",
    "dataset = pseudo_clean(dataset)\n",
    "folds = generate_fold(10, dataset)\n",
    "\n",
    "nb_models = train_naive_bayes(folds)\n",
    "dt_models = train_decision_tree(folds)\n",
    "knn_models = train_knn(5, folds)\n",
    "mlp_models = train_mlp(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def generate_accuracies(models):\n",
    "    for i in range(len(models)):\n",
    "        test_set = folds[i]\n",
    "        test_params, test_lables = seperate(test_set)\n",
    "        predictions = models[i].predict(test_params)\n",
    "        accuracy = accuracy_score(np.array(test_lables), predictions)\n",
    "        print(str(round(accuracy * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes Accuracies:\")\n",
    "generate_accuracies(nb_models)\n",
    "print(\"Decision Tree Accuracies:\")\n",
    "generate_accuracies(dt_models)\n",
    "print(\"K-Nearest Neighbors Accuracies:\")\n",
    "generate_accuracies(knn_models)\n",
    "print(\"Multi-Layer Perceptron Accuracies:\")\n",
    "generate_accuracies(mlp_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dt_models[0], out_file=None) \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
